{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPy+fAk7fY9qYxBBUgOO6h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaewoncutie/ADV-ML-tests/blob/main/fixed_format.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Packages\n"
      ],
      "metadata": {
        "id": "ws2Io9drau4S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cg1LE4_QafxA"
      },
      "outputs": [],
      "source": [
        "!pip install -U scikit-learn nltk pandas matplotlib seaborn scipy wordcloud ipywidgets umap-learn hdbscan"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "AgDVe4U6ay9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import hdbscan\n",
        "from wordcloud import WordCloud\n",
        "import numpy as np\n",
        "import umap\n",
        "from google.colab import files\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "A8b5allOaymq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "tEKqZS6FbI3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lagay mo rito ginawa mo for first part sa data cleaning"
      ],
      "metadata": {
        "id": "uaMl6EqubJex"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BrKa48pebJTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-Processing"
      ],
      "metadata": {
        "id": "pyi9AoLobgt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Limit dataset size for faster processing\n",
        "df = df.sample(n=50000, random_state=42)\n",
        "\n",
        "# Preprocessing Function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove punctuation & numbers\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "df['processed_text'] = df['text'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "WXpv-tU6bmBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize, remove stopwords, and apply lemmatization\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def tokenize_and_lemmatize(text):\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(text) if word not in stop_words])\n",
        "\n",
        "df['filtered_text'] = df['processed_text'].apply(tokenize_and_lemmatize)"
      ],
      "metadata": {
        "id": "geyn2CYFboQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english', max_df=0.7, min_df=5, ngram_range=(1,2))\n",
        "X = vectorizer.fit_transform(df['filtered_text'])"
      ],
      "metadata": {
        "id": "8Eq6h_6ebpU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply TruncatedSVD for Dimensionality Reduction\n",
        "svd = TruncatedSVD(n_components=3600, random_state=42)\n",
        "X_svd = svd.fit_transform(X)"
      ],
      "metadata": {
        "id": "DzjqxEN6b7Ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply UMAP for Non-Linear Dimensionality Reduction\n",
        "umap_model = umap.UMAP(n_components=2, n_neighbors=30, min_dist=0.3, random_state=42)\n",
        "X_umap = umap_model.fit_transform(X_svd)"
      ],
      "metadata": {
        "id": "xHAIYXAfb4uN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize data\n",
        "normalizer = Normalizer()\n",
        "X_normalized = normalizer.fit_transform(X_umap)"
      ],
      "metadata": {
        "id": "g88z6D0jb9Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Means"
      ],
      "metadata": {
        "id": "6rzm-2kEb_zS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Finding the Best K for K-Means -----------------\n",
        "inertia_values = []\n",
        "silhouette_scores = []\n",
        "k_values = range(2, 20)\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X_svd)\n",
        "    inertia_values.append(kmeans.inertia_)\n",
        "    labels_kmeans = kmeans.predict(X_svd)\n",
        "    silhouette_scores.append(silhouette_score(X_svd, labels_kmeans))\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(k_values, inertia_values, marker='o', label='Inertia')\n",
        "plt.plot(k_values, silhouette_scores, marker='s', label='Silhouette Score')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Elbow Method & Silhouette Scores for Optimal K')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "best_k = k_values[np.argmax(silhouette_scores)]\n",
        "kmeans = KMeans(n_clusters=best_k, random_state=42)\n",
        "kmeans_labels = kmeans.fit_predict(X_svd)\n",
        "df['cluster_kmeans'] = kmeans_labels"
      ],
      "metadata": {
        "id": "sgwM9olWcBQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DBScan"
      ],
      "metadata": {
        "id": "_RhCaPUzcIlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Apply DBSCAN -----------------\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=10, metric='euclidean')\n",
        "dbscan_labels = dbscan.fit_predict(X_svd)\n",
        "df['cluster_dbscan'] = dbscan_labels"
      ],
      "metadata": {
        "id": "2aUfYzLUcD0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GMM"
      ],
      "metadata": {
        "id": "gg_U_kUtcN1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Apply GMM (Gaussian Mixture Model) -----------------\n",
        "gmm = GaussianMixture(n_components=best_k, random_state=42)\n",
        "gmm_labels = gmm.fit_predict(X_svd)\n",
        "df['cluster_gmm'] = gmm_labels"
      ],
      "metadata": {
        "id": "zihjmngPcKgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plots"
      ],
      "metadata": {
        "id": "nd5vXHoJcSQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "sns.scatterplot(x=X_umap[:, 0], y=X_umap[:, 1], hue=df['cluster_kmeans'], palette='tab20', ax=axes[0, 0])\n",
        "axes[0, 0].set_title(f'K-Means Clustering (k={best_k})')\n",
        "\n",
        "sns.scatterplot(x=X_umap[:, 0], y=X_umap[:, 1], hue=df['cluster_dbscan'], palette='tab20', ax=axes[0, 1])\n",
        "axes[0, 1].set_title('DBSCAN Clustering')\n",
        "\n",
        "sns.scatterplot(x=X_umap[:, 0], y=X_umap[:, 1], hue=df['cluster_gmm'], palette='tab20', ax=axes[1, 0])\n",
        "axes[1, 0].set_title('GMM Clustering')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Clustering process completed.\")"
      ],
      "metadata": {
        "id": "Rzerrgl-cNgB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}